{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aO5rRXvw1z6X"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkiIT5l6LPmM"
   },
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "torch_dtype = None\n",
    "BASE=\"Qwen/Qwen2.5-3B-Instruct\"\n",
    "ADAPTER = \"abdelalem33/coder\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype = torch_dtype\n",
    ").to(device)\n",
    "\n",
    "model_with_adaptor = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    ADAPTER,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSsnDYhPLPe8"
   },
   "outputs": [],
   "source": [
    "#to see  model arch with lora\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1cNx_WELPWF"
   },
   "outputs": [],
   "source": [
    "def generate_resp(message):\n",
    "    message=[{\"role\":\"system\",\"content\":\"write just python code\"},{\"role\": \"user\", \"content\": messages}]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    model_inputs = tokenizer([text],\n",
    "                             return_tensors=\"pt\",\n",
    "                             padding=True).to(device)\n",
    "\n",
    "    generated_ids = model_with_adaptor.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False, top_k=None, temperature=None, top_p=None,\n",
    "\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):]\n",
    "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKqCS9OWLPOj"
   },
   "outputs": [],
   "source": [
    "msg=\"convert list [1,2,5] to one integer\"\n",
    "print(generate_resp(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGKlIbcfLWoQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMaAegQGq2OuLdkBYDJjaP4",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
