{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "NDO2aDUpM9OZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2025-11-28T15:21:29.346486Z",
          "iopub.status.busy": "2025-11-28T15:21:29.346238Z",
          "iopub.status.idle": "2025-11-28T15:23:01.612775Z",
          "shell.execute_reply": "2025-11-28T15:23:01.611786Z",
          "shell.execute_reply.started": "2025-11-28T15:21:29.346463Z"
        },
        "id": "Owmtj9hSM9Oa",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "0d8b88f2-0732-4e8b-d60d-d55ebbecf2f1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.57.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.3.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes, accelerate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.9.0\n",
            "    Uninstalling accelerate-1.9.0:\n",
            "      Successfully uninstalled accelerate-1.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
            "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
            "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.12.0 bitsandbytes-0.48.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install -q json_repair\n",
        "!pip install -q wandb\n",
        "!pip install -q pydantic\n",
        "!pip install -qU numpy\n",
        "!pip install -U transformers accelerate bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "35447c4d60e34997a3f123af412196f0"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-11-28T15:26:14.359786Z",
          "iopub.status.busy": "2025-11-28T15:26:14.359476Z",
          "iopub.status.idle": "2025-11-28T15:26:14.373945Z",
          "shell.execute_reply": "2025-11-28T15:26:14.373232Z",
          "shell.execute_reply.started": "2025-11-28T15:26:14.359764Z"
        },
        "id": "gct9bu57M9Ob",
        "outputId": "d96649c1-9638-46db-9a9b-d75f05453138",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35447c4d60e34997a3f123af412196f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:25:10.61092Z",
          "iopub.status.busy": "2025-11-28T15:25:10.610625Z",
          "iopub.status.idle": "2025-11-28T15:25:18.838936Z",
          "shell.execute_reply": "2025-11-28T15:25:18.83819Z",
          "shell.execute_reply.started": "2025-11-28T15:25:10.6109Z"
        },
        "id": "wSuup5jdM9Oc",
        "outputId": "0efc9a1a-747f-409d-9c08-aed1b963fc22",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login(key='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:25:06.916029Z",
          "iopub.status.busy": "2025-11-28T15:25:06.915193Z",
          "iopub.status.idle": "2025-11-28T15:25:06.922446Z",
          "shell.execute_reply": "2025-11-28T15:25:06.921552Z",
          "shell.execute_reply.started": "2025-11-28T15:25:06.915993Z"
        },
        "id": "-WJrWzIRM9Oc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from os.path import join\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "import torch\n",
        "\n",
        "base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "device = \"cuda\"\n",
        "torch_dtype = None\n",
        "\n",
        "data_dir=\"/kaggle/working/\"\n",
        "\n",
        "def parse_json(text):\n",
        "    \"\"\"\n",
        "    parse the output from llm to valid json\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return json_repair.loads(text)\n",
        "    except:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:26:53.887681Z",
          "iopub.status.busy": "2025-11-28T15:26:53.88737Z",
          "iopub.status.idle": "2025-11-28T15:26:55.466252Z",
          "shell.execute_reply": "2025-11-28T15:26:55.465537Z",
          "shell.execute_reply.started": "2025-11-28T15:26:53.887657Z"
        },
        "id": "jQ6Ize_3M9Oc",
        "outputId": "3df9ea22-0aac-46db-ca28-532e0f7e2b7f",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-11-28 15:26:53--  http://www.phontron.com/download/conala-corpus-v1.1.zip\n",
            "Resolving www.phontron.com (www.phontron.com)... 173.236.247.185\n",
            "Connecting to www.phontron.com (www.phontron.com)|173.236.247.185|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52105440 (50M) [application/zip]\n",
            "Saving to: ‘conala-corpus-v1.1.zip’\n",
            "\n",
            "conala-corpus-v1.1. 100%[===================>]  49.69M  41.0MB/s    in 1.2s    \n",
            "\n",
            "2025-11-28 15:26:55 (41.0 MB/s) - ‘conala-corpus-v1.1.zip’ saved [52105440/52105440]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://www.phontron.com/download/conala-corpus-v1.1.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:26:55.467887Z",
          "iopub.status.busy": "2025-11-28T15:26:55.46765Z",
          "iopub.status.idle": "2025-11-28T15:26:56.925554Z",
          "shell.execute_reply": "2025-11-28T15:26:56.924795Z",
          "shell.execute_reply.started": "2025-11-28T15:26:55.467865Z"
        },
        "id": "mTw776bcM9Od",
        "outputId": "fb5e3895-c47b-40f2-cd8c-1a56d583e4ea",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /kaggle/working/conala-corpus-v1.1.zip\n",
            "   creating: conala-corpus/\n",
            "  inflating: conala-corpus/conala-mined.jsonl  \n",
            "  inflating: conala-corpus/conala-train.json  \n",
            "  inflating: conala-corpus/conala-test.json  \n"
          ]
        }
      ],
      "source": [
        "!unzip /kaggle/working/conala-corpus-v1.1.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:26:56.958796Z",
          "iopub.status.busy": "2025-11-28T15:26:56.958557Z",
          "iopub.status.idle": "2025-11-28T15:26:56.983028Z",
          "shell.execute_reply": "2025-11-28T15:26:56.982258Z",
          "shell.execute_reply.started": "2025-11-28T15:26:56.958773Z"
        },
        "id": "YxEs_487M9Od",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train = pd.read_json('/kaggle/working/conala-corpus/conala-train.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:26:57.283745Z",
          "iopub.status.busy": "2025-11-28T15:26:57.283169Z",
          "iopub.status.idle": "2025-11-28T15:26:57.306087Z",
          "shell.execute_reply": "2025-11-28T15:26:57.305479Z",
          "shell.execute_reply.started": "2025-11-28T15:26:57.283719Z"
        },
        "id": "PsX6QTo2M9Od",
        "outputId": "1502f92b-423b-44a8-e09b-f72a2e578aa0",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>intent</th>\n",
              "      <th>rewritten_intent</th>\n",
              "      <th>snippet</th>\n",
              "      <th>question_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How to convert a list of multiple integers int...</td>\n",
              "      <td>Concatenate elements of a list 'x' of multiple...</td>\n",
              "      <td>sum(d * 10 ** i for i, d in enumerate(x[::-1]))</td>\n",
              "      <td>41067960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How to convert a list of multiple integers int...</td>\n",
              "      <td>convert a list of integers into a single integer</td>\n",
              "      <td>r = int(''.join(map(str, x)))</td>\n",
              "      <td>41067960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>how to convert a datetime string back to datet...</td>\n",
              "      <td>convert a DateTime string back to a DateTime o...</td>\n",
              "      <td>datetime.strptime('2010-11-13 10:33:54.227806'...</td>\n",
              "      <td>4170655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Averaging the values in a dictionary based on ...</td>\n",
              "      <td>get the average of a list values for each key ...</td>\n",
              "      <td>[(i, sum(j) / len(j)) for i, j in list(d.items...</td>\n",
              "      <td>29565452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>zip lists in python</td>\n",
              "      <td>zip two lists `[1, 2]` and `[3, 4]` into a lis...</td>\n",
              "      <td>zip([1, 2], [3, 4])</td>\n",
              "      <td>13704860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2374</th>\n",
              "      <td>how to change [1,2,3,4] to '1234' using python</td>\n",
              "      <td>join list of numbers `[1,2,3,4] ` to string of...</td>\n",
              "      <td>\"\"\"\"\"\".join([1, 2, 3, 4])</td>\n",
              "      <td>2597932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2375</th>\n",
              "      <td>Delete every non utf-8 symbols froms string</td>\n",
              "      <td>delete every non `utf-8` characters from a str...</td>\n",
              "      <td>line = line.decode('utf-8', 'ignore').encode('...</td>\n",
              "      <td>26541968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2376</th>\n",
              "      <td>How to execute a command in the terminal from ...</td>\n",
              "      <td>execute a command `command ` in the terminal f...</td>\n",
              "      <td>os.system(command)</td>\n",
              "      <td>33065588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2377</th>\n",
              "      <td>Python MySQL Parameterized Queries</td>\n",
              "      <td>MySQL execute query 'SELECT * FROM foo WHERE b...</td>\n",
              "      <td>c.execute('SELECT * FROM foo WHERE bar = %s AN...</td>\n",
              "      <td>775296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2378</th>\n",
              "      <td>Convert a string to datetime object in python</td>\n",
              "      <td>Parse string `datestr` into a datetime object ...</td>\n",
              "      <td>dateobj = datetime.datetime.strptime(datestr, ...</td>\n",
              "      <td>5868374</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2379 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 intent  \\\n",
              "0     How to convert a list of multiple integers int...   \n",
              "1     How to convert a list of multiple integers int...   \n",
              "2     how to convert a datetime string back to datet...   \n",
              "3     Averaging the values in a dictionary based on ...   \n",
              "4                                   zip lists in python   \n",
              "...                                                 ...   \n",
              "2374     how to change [1,2,3,4] to '1234' using python   \n",
              "2375        Delete every non utf-8 symbols froms string   \n",
              "2376  How to execute a command in the terminal from ...   \n",
              "2377                 Python MySQL Parameterized Queries   \n",
              "2378      Convert a string to datetime object in python   \n",
              "\n",
              "                                       rewritten_intent  \\\n",
              "0     Concatenate elements of a list 'x' of multiple...   \n",
              "1      convert a list of integers into a single integer   \n",
              "2     convert a DateTime string back to a DateTime o...   \n",
              "3     get the average of a list values for each key ...   \n",
              "4     zip two lists `[1, 2]` and `[3, 4]` into a lis...   \n",
              "...                                                 ...   \n",
              "2374  join list of numbers `[1,2,3,4] ` to string of...   \n",
              "2375  delete every non `utf-8` characters from a str...   \n",
              "2376  execute a command `command ` in the terminal f...   \n",
              "2377  MySQL execute query 'SELECT * FROM foo WHERE b...   \n",
              "2378  Parse string `datestr` into a datetime object ...   \n",
              "\n",
              "                                                snippet  question_id  \n",
              "0       sum(d * 10 ** i for i, d in enumerate(x[::-1]))     41067960  \n",
              "1                         r = int(''.join(map(str, x)))     41067960  \n",
              "2     datetime.strptime('2010-11-13 10:33:54.227806'...      4170655  \n",
              "3     [(i, sum(j) / len(j)) for i, j in list(d.items...     29565452  \n",
              "4                                   zip([1, 2], [3, 4])     13704860  \n",
              "...                                                 ...          ...  \n",
              "2374                          \"\"\"\"\"\".join([1, 2, 3, 4])      2597932  \n",
              "2375  line = line.decode('utf-8', 'ignore').encode('...     26541968  \n",
              "2376                                 os.system(command)     33065588  \n",
              "2377  c.execute('SELECT * FROM foo WHERE bar = %s AN...       775296  \n",
              "2378  dateobj = datetime.datetime.strptime(datestr, ...      5868374  \n",
              "\n",
              "[2379 rows x 4 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:27:01.031103Z",
          "iopub.status.busy": "2025-11-28T15:27:01.030437Z",
          "iopub.status.idle": "2025-11-28T15:27:01.048856Z",
          "shell.execute_reply": "2025-11-28T15:27:01.048157Z",
          "shell.execute_reply.started": "2025-11-28T15:27:01.031078Z"
        },
        "id": "jMj1-4bDM9Oe",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('/kaggle/working/conala-corpus/conala-train.json', 'r') as json_file:\n",
        "   train_data = json.load(json_file)\n",
        "\n",
        "with open('/kaggle/working/fine_tune_train.jsonl', 'w') as jsonl_file:\n",
        "   for entry in train_data:\n",
        "       jsonl_file.write(json.dumps(entry) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:27:01.243565Z",
          "iopub.status.busy": "2025-11-28T15:27:01.242916Z",
          "iopub.status.idle": "2025-11-28T15:27:34.757541Z",
          "shell.execute_reply": "2025-11-28T15:27:34.756693Z",
          "shell.execute_reply.started": "2025-11-28T15:27:01.243544Z"
        },
        "id": "4PDzOKH_M9Oe",
        "outputId": "daba43f0-9500-415f-e241-cfaad6eba941",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 453, done.\u001b[K\n",
            "remote: Counting objects: 100% (453/453), done.\u001b[K\n",
            "remote: Compressing objects: 100% (346/346), done.\u001b[K\n",
            "remote: Total 453 (delta 118), reused 336 (delta 89), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (453/453), 5.15 MiB | 28.52 MiB/s, done.\n",
            "Resolving deltas: 100% (118/118), done.\n",
            "Obtaining file:///kaggle/working/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets<=4.0.0,>=2.16.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting accelerate<=1.11.0,>=1.3.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: peft<=0.17.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.16.0)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: gradio<=5.45.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (5.38.1)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (3.7.2)\n",
            "Collecting tyro<0.9.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.8.1)\n",
            "Collecting numpy<2.0.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (2.2.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (1.15.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.9.0)\n",
            "Collecting modelscope>=1.14.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading modelscope-1.32.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: hf-transfer in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.1.9)\n",
            "Requirement already satisfied: safetensors<=0.5.3 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.5.3)\n",
            "Collecting fire (from llamafactory==0.9.4.dev0)\n",
            "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (6.33.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (6.0.3)\n",
            "Collecting pydantic<=2.10.6 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.35.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.116.1)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (3.0.3)\n",
            "Collecting av (from llamafactory==0.9.4.dev0)\n",
            "  Downloading av-16.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.11.0)\n",
            "Requirement already satisfied: propcache!=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.4.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (7.1.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (19.0.1)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.6.0)\n",
            "Collecting multiprocess<0.70.17 (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (22.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.1.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.11.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.12.5)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (4.15.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (2.9.0.post0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from modelscope>=1.14.0->llamafactory==0.9.4.dev0) (75.2.0)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.11/dist-packages (from modelscope>=1.14.0->llamafactory==0.9.4.dev0) (2.5.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.4.dev0) (0.7.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic<=2.10.6->llamafactory==0.9.4.dev0)\n",
            "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0->llamafactory==0.9.4.dev0) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0->llamafactory==0.9.4.dev0) (0.22.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.4.dev0) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.4.dev0) (14.2.0)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading shtab-1.8.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.4.dev0) (8.3.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.4.dev0) (0.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.4.dev0) (3.1.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.1.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->llamafactory==0.9.4.dev0) (4.9.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.13.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.0.9)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (1.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.4.dev0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.4.dev0) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (2.19.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.4.dev0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.4.dev0) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (6.7.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.22.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.4.dev0) (2.23)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (0.1.2)\n",
            "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading modelscope-1.32.0-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m121.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-16.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (40.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.8.0-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.4.dev0-0.editable-py3-none-any.whl size=28945 sha256=798ac84e86bbd9a3124010df618bd64e0cc63fc3415e227c1dc54b966848abea\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-474ppd3q/wheels/96/d8/b2/8fc665ed70525080a50f3ff8538833c6f74cd48eb82195d0f8\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: shtab, pydantic-core, numpy, fsspec, fire, dill, av, pydantic, multiprocess, modelscope, tyro, transformers, datasets, accelerate, trl, llamafactory\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.41.5\n",
            "    Uninstalling pydantic_core-2.41.5:\n",
            "      Successfully uninstalled pydantic_core-2.41.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.5\n",
            "    Uninstalling numpy-2.3.5:\n",
            "      Successfully uninstalled numpy-2.3.5\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.10.0\n",
            "    Uninstalling fsspec-2025.10.0:\n",
            "      Successfully uninstalled fsspec-2025.10.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.4.0\n",
            "    Uninstalling dill-0.4.0:\n",
            "      Successfully uninstalled dill-0.4.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.12.4\n",
            "    Uninstalling pydantic-2.12.4:\n",
            "      Successfully uninstalled pydantic-2.12.4\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.18\n",
            "    Uninstalling multiprocess-0.70.18:\n",
            "      Successfully uninstalled multiprocess-0.70.18\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.3\n",
            "    Uninstalling transformers-4.57.3:\n",
            "      Successfully uninstalled transformers-4.57.3\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.4.1\n",
            "    Uninstalling datasets-4.4.1:\n",
            "      Successfully uninstalled datasets-4.4.1\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.12.0\n",
            "    Uninstalling accelerate-1.12.0:\n",
            "      Successfully uninstalled accelerate-1.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
            "sigstore-models 0.0.5 requires pydantic>=2.11.7, but you have pydantic 2.10.6 which is incompatible.\n",
            "a2a-sdk 0.3.10 requires pydantic>=2.11.3, but you have pydantic 2.10.6 which is incompatible.\n",
            "mcp 1.20.0 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 2.10.6 which is incompatible.\n",
            "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
            "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\n",
            "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
            "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.11.0 av-16.0.1 datasets-4.0.0 dill-0.3.8 fire-0.7.1 fsspec-2025.3.0 llamafactory-0.9.4.dev0 modelscope-1.32.0 multiprocess-0.70.16 numpy-1.26.4 pydantic-2.10.6 pydantic-core-2.27.2 shtab-1.8.0 transformers-4.57.1 trl-0.9.6 tyro-0.8.14\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "!cd LLaMA-Factory && pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:27:34.759825Z",
          "iopub.status.busy": "2025-11-28T15:27:34.759291Z",
          "iopub.status.idle": "2025-11-28T15:27:34.766407Z",
          "shell.execute_reply": "2025-11-28T15:27:34.765779Z",
          "shell.execute_reply.started": "2025-11-28T15:27:34.759799Z"
        },
        "id": "NWYxwXs8M9Oe",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Intent(BaseModel):\n",
        "    enhanced_intent: str = Field(..., min_length=5, max_length=300,\n",
        "                             description=\"The enhanced version of Intent.\")\n",
        "\n",
        "\n",
        "class Code(BaseModel):\n",
        "    Code: str = Field(..., min_length=5, max_length=300,\n",
        "                             description=\"The Output code in Python.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:27:34.767581Z",
          "iopub.status.busy": "2025-11-28T15:27:34.767311Z",
          "iopub.status.idle": "2025-11-28T15:27:34.791556Z",
          "shell.execute_reply": "2025-11-28T15:27:34.790808Z",
          "shell.execute_reply.started": "2025-11-28T15:27:34.767558Z"
        },
        "id": "fHWNmh8nM9Oe",
        "outputId": "6fd089d0-4fce-4c89-c33a-dadc3bd3ad55",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/kaggle/working/\n"
          ]
        }
      ],
      "source": [
        "print(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:27:41.151816Z",
          "iopub.status.busy": "2025-11-28T15:27:41.151077Z",
          "iopub.status.idle": "2025-11-28T15:27:41.7796Z",
          "shell.execute_reply": "2025-11-28T15:27:41.779Z",
          "shell.execute_reply.started": "2025-11-28T15:27:41.151788Z"
        },
        "id": "GHFa6g_FM9Oe",
        "outputId": "7651497c-c3dd-432f-cba2-7f23ff6d2d86",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2379/2379 [00:00<00:00, 3844.16it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "save_to = join(data_dir, \"train.jsonl\")\n",
        "\n",
        "ix = 0\n",
        "for item in tqdm(train_data):\n",
        "    with open(save_to, \"a\", encoding=\"utf8\") as dest:\n",
        "        dest.write(json.dumps({\n",
        "            \"id\": ix,\n",
        "            \"intent\": item['intent'].strip(),\n",
        "            \"task\": \"write the enhanced version of the intent into a JSON.\",\n",
        "            \"output_scheme\": json.dumps( Intent.model_json_schema(), ensure_ascii=False ),\n",
        "            \"response\": item['rewritten_intent'],\n",
        "        }, ensure_ascii=False, default=str)  + \"\\n\" )\n",
        "\n",
        "    ix += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:27:43.404139Z",
          "iopub.status.busy": "2025-11-28T15:27:43.403516Z",
          "iopub.status.idle": "2025-11-28T15:27:44.025095Z",
          "shell.execute_reply": "2025-11-28T15:27:44.024428Z",
          "shell.execute_reply.started": "2025-11-28T15:27:43.404115Z"
        },
        "id": "VeXRuYN0M9Of",
        "outputId": "51b875bd-418a-4895-fd0f-b1693e16928a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2379/2379 [00:00<00:00, 3870.51it/s]\n"
          ]
        }
      ],
      "source": [
        "for item in tqdm(train_data):\n",
        "    with open(save_to, \"a\", encoding=\"utf8\") as dest:\n",
        "        dest.write(json.dumps({\n",
        "            \"id\": ix,\n",
        "            \"intent\": item['intent'].strip(),\n",
        "            \"task\": \"write the needed code in python into a JSON.\",\n",
        "            \"output_scheme\": json.dumps( Code.model_json_schema(), ensure_ascii=False ),\n",
        "            \"response\": item['snippet'],\n",
        "        }, ensure_ascii=False, default=str)  + \"\\n\" )\n",
        "\n",
        "    ix += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:27:44.891025Z",
          "iopub.status.busy": "2025-11-28T15:27:44.89024Z",
          "iopub.status.idle": "2025-11-28T15:27:44.942786Z",
          "shell.execute_reply": "2025-11-28T15:27:44.942112Z",
          "shell.execute_reply.started": "2025-11-28T15:27:44.890989Z"
        },
        "id": "cqs927bLM9Of",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "intent_system_msg=\"\\n\".join([\n",
        "            \"You are a technical intent rewriter preparing specifications for LLM code generation.\",\n",
        "            \"Your rewritten intent must provide ALL information needed for an LLM to generate correct code WITHOUT additional context.\",\n",
        "            \"Essential elements to include:\",\n",
        "            \"1. Variable names - Give explicit names (x, s, d, lst) so the LLM knows what to use\",\n",
        "            \"2. Data structures - Specify list, dict, set, string, tuple, etc.\",\n",
        "            \"3. Data types - State integers, strings, floats, booleans, etc.\",\n",
        "            \"4. Operation - Use precise verbs: concatenate, filter, transform, merge, extract, aggregate, sort\",\n",
        "            \"5. Output format - Describe what the result should be\",\n",
        "            \"6. Implicit details - Order preservation, empty case handling, type conversions\",\n",
        "            \"Bad: 'combine numbers in a list'\",\n",
        "            \"Good: 'Concatenate elements of a list 'x' of multiple integers to a single integer'\",\n",
        "            \"Why? The good version tells the LLM:\",\n",
        "            \"- Variable name: 'x'\",\n",
        "            \"- Input type: list of integers\",\n",
        "            \"- Operation: concatenate (not sum, not join with delimiter)\",\n",
        "            \"- Output type: single integer\"\n",
        "        ])\n",
        "\n",
        "code_system_msg =\"\\n\".join([\n",
        "            \"You are an expert Python code generator.\",\n",
        "            \"You will receive a technical specification describing a programming task.\",\n",
        "            \"Generate clean, efficient Python code that exactly implements the specification.\",\n",
        "            \"Follow these rules:\",\n",
        "            \"1. Use the exact variable names specified in the input\",\n",
        "            \"2. Match the data structures and types mentioned\",\n",
        "            \"3. Implement the precise operation described\",\n",
        "            \"4. Generate only the code expression or statement - no explanations, no comments, no markdown\",\n",
        "            \"5. Write concise, Pythonic code using appropriate built-in functions and list comprehensions\",\n",
        "            \"6. Handle the exact input/output format specified\",\n",
        "            \"Do not add any introduction, explanation, or conclusion.\",\n",
        "            \"Output only executable Python code.\"\n",
        "        ])\n",
        "\n",
        "\n",
        "llm_finetuning_data = []\n",
        "data_path=\"/kaggle/working/train.jsonl\"\n",
        "for line in open(data_path, \"r\"):\n",
        "    if line.strip()==\"\":\n",
        "        continue\n",
        "\n",
        "    rec =json.loads(line.strip())\n",
        "\n",
        "    if rec['task']==\"write the enhanced version of the intent into a JSON.\":\n",
        "        system_message = intent_system_msg\n",
        "    else:\n",
        "        system_message = code_system_msg\n",
        "\n",
        "    llm_finetuning_data.append({\n",
        "        \"system\": system_message,\n",
        "\n",
        "        \"instruction\":\"\\n\".join([\n",
        "            \"#Intent:\",\n",
        "            rec[\"intent\"],\n",
        "            \"\",\n",
        "\n",
        "            \"#Task:\",\n",
        "            rec[\"task\"],\n",
        "            \"\",\n",
        "\n",
        "            \"#Output Scheme:\",\n",
        "            rec[\"output_scheme\"],\n",
        "            \"\",\n",
        "\n",
        "            \"#Output Json:\",\n",
        "            \"```json\"\n",
        "        ]),\n",
        "\n",
        "        \"input\":\"\",    # the task\n",
        "\n",
        "        \"output\":\"\\n\".join([\n",
        "            \"```json\",\n",
        "            json.dumps(rec[\"response\"], ensure_ascii=False, default=str),\n",
        "            \"```\"\n",
        "        ]),\n",
        "\n",
        "        \"history\":[]\n",
        "    })\n",
        "\n",
        "random.Random(101).shuffle(llm_finetuning_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:27:46.848737Z",
          "iopub.status.busy": "2025-11-28T15:27:46.848444Z",
          "iopub.status.idle": "2025-11-28T15:27:46.853688Z",
          "shell.execute_reply": "2025-11-28T15:27:46.853009Z",
          "shell.execute_reply.started": "2025-11-28T15:27:46.848714Z"
        },
        "id": "87RXZ5YVM9Of",
        "outputId": "7f33e82e-420b-4a6e-f3cf-7ffbdae2f878",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4758"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(llm_finetuning_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:27:48.911964Z",
          "iopub.status.busy": "2025-11-28T15:27:48.911485Z",
          "iopub.status.idle": "2025-11-28T15:27:48.992357Z",
          "shell.execute_reply": "2025-11-28T15:27:48.991796Z",
          "shell.execute_reply.started": "2025-11-28T15:27:48.911937Z"
        },
        "id": "bSaY96oqM9Of",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_sample_sz = 4000\n",
        "\n",
        "train_ds = llm_finetuning_data[:train_sample_sz]\n",
        "ev_ds = llm_finetuning_data[train_sample_sz:]\n",
        "\n",
        "os.makedirs(join(data_dir, \"datasets\", \"llamafactory-finetune-data\"), exist_ok=True)\n",
        "\n",
        "with open(join(data_dir, \"datasets\", \"llamafactory-finetune-data\", \"train.jsonl\"), \"w\", encoding=\"utf8\") as dest:\n",
        "    for item in train_ds:\n",
        "        dest.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "with open(join(data_dir, \"datasets\", \"llamafactory-finetune-data\", \"val.jsonl\"), \"w\", encoding=\"utf8\") as dest:\n",
        "    for item in ev_ds:\n",
        "        dest.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLs5S5KZM9Of",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# \"code_finetune_train\": {\n",
        "#         \"file_name\": \"/kaggle/working/datasets/llamafactory-finetune-data/train.jsonl\",\n",
        "#         \"columns\": {\n",
        "#             \"prompt\": \"instruction\",\n",
        "#             \"query\": \"input\",\n",
        "#             \"response\": \"output\",\n",
        "#             \"system\": \"system\",\n",
        "#             \"history\": \"history\"\n",
        "#         }\n",
        "#     },\n",
        "# \"code_finetune_val\": {\n",
        "#         \"file_name\": \"/kaggle/working/datasets/llamafactory-finetune-data/val.jsonl\",\n",
        "#         \"columns\": {\n",
        "#             \"prompt\": \"instruction\",\n",
        "#             \"query\": \"input\",\n",
        "#             \"response\": \"output\",\n",
        "#             \"system\": \"system\",\n",
        "#             \"history\": \"history\"\n",
        "#         }\n",
        "#     }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:27:52.061696Z",
          "iopub.status.busy": "2025-11-28T15:27:52.061186Z",
          "iopub.status.idle": "2025-11-28T15:27:52.069251Z",
          "shell.execute_reply": "2025-11-28T15:27:52.068647Z",
          "shell.execute_reply.started": "2025-11-28T15:27:52.061673Z"
        },
        "id": "dDXliaEEM9Of",
        "outputId": "aebf1201-1905-4fac-becd-eb48368cf1bf",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✨ Done! JSON updated successfully.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "json_path = \"/kaggle/working/LLaMA-Factory/data/dataset_info.json\"\n",
        "\n",
        "# Load existing JSON\n",
        "with open(json_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Your datasets to add\n",
        "new_entries = {\n",
        "    \"code_finetune_train\": {\n",
        "        \"file_name\": \"/kaggle/working/datasets/llamafactory-finetune-data/train.jsonl\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"query\": \"input\",\n",
        "            \"response\": \"output\",\n",
        "            \"system\": \"system\",\n",
        "            \"history\": \"history\"\n",
        "        }\n",
        "    },\n",
        "    \"code_finetune_val\": {\n",
        "        \"file_name\": \"/kaggle/working/datasets/llamafactory-finetune-data/val.jsonl\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"query\": \"input\",\n",
        "            \"response\": \"output\",\n",
        "            \"system\": \"system\",\n",
        "            \"history\": \"history\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Update and save back\n",
        "data.update(new_entries)\n",
        "\n",
        "with open(json_path, \"w\") as f:\n",
        "    json.dump(data, f, indent=4)\n",
        "\n",
        "print(\"✨ Done! JSON updated successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2025-11-28T15:27:54.368237Z",
          "iopub.status.busy": "2025-11-28T15:27:54.367612Z",
          "iopub.status.idle": "2025-11-28T15:27:54.374228Z",
          "shell.execute_reply": "2025-11-28T15:27:54.373398Z",
          "shell.execute_reply.started": "2025-11-28T15:27:54.368213Z"
        },
        "id": "U2JhcxYUM9Og",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "375d3c62-aa3f-4e8e-8e59-66d2a3da8175",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"identity\": {\n",
            "        \"file_name\": \"identity.json\"\n",
            "    },\n",
            "    \"alpaca_en_demo\": {\n",
            "        \"file_name\": \"alpaca_en_demo.json\"\n",
            "    },\n",
            "    \"alpaca_zh_demo\": {\n",
            "        \"file_name\": \"alpaca_zh_demo.json\"\n",
            "    },\n",
            "    \"glaive_toolcall_en_demo\": {\n",
            "        \"file_name\": \"glaive_toolcall_en_demo.json\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"conversations\",\n",
            "            \"tools\": \"tools\"\n",
            "        }\n",
            "    },\n",
            "    \"glaive_toolcall_zh_demo\": {\n",
            "        \"file_name\": \"glaive_toolcall_zh_demo.json\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"conversations\",\n",
            "            \"tools\": \"tools\"\n",
            "        }\n",
            "    },\n",
            "    \"mllm_demo\": {\n",
            "        \"file_name\": \"mllm_demo.json\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"messages\",\n",
            "            \"images\": \"images\"\n",
            "        },\n",
            "        \"tags\": {\n",
            "            \"role_tag\": \"role\",\n",
            "            \"content_tag\": \"content\",\n",
            "            \"user_tag\": \"user\",\n",
            "            \"assistant_tag\": \"assistant\"\n",
            "        }\n",
            "    },\n",
            "    \"mllm_audio_demo\": {\n",
            "        \"file_name\": \"mllm_audio_demo.json\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"messages\",\n",
            "            \"audios\": \"audios\"\n",
            "        },\n",
            "        \"tags\": {\n",
            "            \"role_tag\": \"role\",\n",
            "            \"content_tag\": \"content\",\n",
            "            \"user_tag\": \"user\",\n",
            "            \"assistant_tag\": \"assistant\"\n",
            "        }\n",
            "    },\n",
            "    \"mllm_video_demo\": {\n",
            "        \"file_name\": \"mllm_video_demo.json\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"messages\",\n",
            "            \"videos\": \"videos\"\n",
            "        },\n",
            "        \"tags\": {\n",
            "            \"role_tag\": \"role\",\n",
            "            \"content_tag\": \"content\",\n",
            "            \"user_tag\": \"user\",\n",
            "            \"assistant_tag\": \"assistant\"\n",
            "        }\n",
            "    },\n",
            "    \"mllm_video_audio_demo\": {\n",
            "        \"file_name\": \"mllm_video_audio_demo.json\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"messages\",\n",
            "            \"videos\": \"videos\",\n",
            "            \"audios\": \"audios\"\n",
            "        },\n",
            "        \"tags\": {\n",
            "            \"role_tag\": \"role\",\n",
            "            \"content_tag\": \"content\",\n",
            "            \"user_tag\": \"user\",\n",
            "            \"assistant_tag\": \"assistant\"\n",
            "        }\n",
            "    },\n",
            "    \"alpaca_en\": {\n",
            "        \"hf_hub_url\": \"llamafactory/alpaca_en\",\n",
            "        \"ms_hub_url\": \"llamafactory/alpaca_en\",\n",
            "        \"om_hub_url\": \"HaM/alpaca_en\"\n",
            "    },\n",
            "    \"alpaca_zh\": {\n",
            "        \"hf_hub_url\": \"llamafactory/alpaca_zh\",\n",
            "        \"ms_hub_url\": \"llamafactory/alpaca_zh\"\n",
            "    },\n",
            "    \"alpaca_gpt4_en\": {\n",
            "        \"hf_hub_url\": \"llamafactory/alpaca_gpt4_en\",\n",
            "        \"ms_hub_url\": \"llamafactory/alpaca_gpt4_en\"\n",
            "    },\n",
            "    \"alpaca_gpt4_zh\": {\n",
            "        \"hf_hub_url\": \"llamafactory/alpaca_gpt4_zh\",\n",
            "        \"ms_hub_url\": \"llamafactory/alpaca_gpt4_zh\",\n",
            "        \"om_hub_url\": \"State_Cloud/alpaca-gpt4-data-zh\"\n",
            "    },\n",
            "    \"glaive_toolcall_en\": {\n",
            "        \"hf_hub_url\": \"llamafactory/glaive_toolcall_en\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"conversations\",\n",
            "            \"tools\": \"tools\"\n",
            "        }\n",
            "    },\n",
            "    \"glaive_toolcall_zh\": {\n",
            "        \"hf_hub_url\": \"llamafactory/glaive_toolcall_zh\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"conversations\",\n",
            "            \"tools\": \"tools\"\n",
            "        }\n",
            "    },\n",
            "    \"lima\": {\n",
            "        \"hf_hub_url\": \"llamafactory/lima\",\n",
            "        \"formatting\": \"sharegpt\"\n",
            "    },\n",
            "    \"guanaco\": {\n",
            "        \"hf_hub_url\": \"JosephusCheung/GuanacoDataset\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/GuanacoDataset\"\n",
            "    },\n",
            "    \"belle_2m\": {\n",
            "        \"hf_hub_url\": \"BelleGroup/train_2M_CN\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/train_2M_CN\"\n",
            "    },\n",
            "    \"belle_1m\": {\n",
            "        \"hf_hub_url\": \"BelleGroup/train_1M_CN\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/train_1M_CN\"\n",
            "    },\n",
            "    \"belle_0.5m\": {\n",
            "        \"hf_hub_url\": \"BelleGroup/train_0.5M_CN\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/train_0.5M_CN\"\n",
            "    },\n",
            "    \"belle_dialog\": {\n",
            "        \"hf_hub_url\": \"BelleGroup/generated_chat_0.4M\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/generated_chat_0.4M\"\n",
            "    },\n",
            "    \"belle_math\": {\n",
            "        \"hf_hub_url\": \"BelleGroup/school_math_0.25M\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/school_math_0.25M\"\n",
            "    },\n",
            "    \"open_platypus\": {\n",
            "        \"hf_hub_url\": \"garage-bAInd/Open-Platypus\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/Open-Platypus\"\n",
            "    },\n",
            "    \"codealpaca\": {\n",
            "        \"hf_hub_url\": \"sahil2801/CodeAlpaca-20k\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/CodeAlpaca-20k\"\n",
            "    },\n",
            "    \"alpaca_cot\": {\n",
            "        \"hf_hub_url\": \"QingyiSi/Alpaca-CoT\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/Alpaca-CoT\"\n",
            "    },\n",
            "    \"openorca\": {\n",
            "        \"hf_hub_url\": \"Open-Orca/OpenOrca\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/OpenOrca\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"question\",\n",
            "            \"response\": \"response\",\n",
            "            \"system\": \"system_prompt\"\n",
            "        }\n",
            "    },\n",
            "    \"slimorca\": {\n",
            "        \"hf_hub_url\": \"Open-Orca/SlimOrca\",\n",
            "        \"formatting\": \"sharegpt\"\n",
            "    },\n",
            "    \"mathinstruct\": {\n",
            "        \"hf_hub_url\": \"TIGER-Lab/MathInstruct\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/MathInstruct\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"instruction\",\n",
            "            \"response\": \"output\"\n",
            "        }\n",
            "    },\n",
            "    \"firefly\": {\n",
            "        \"hf_hub_url\": \"YeungNLP/firefly-train-1.1M\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"input\",\n",
            "            \"response\": \"target\"\n",
            "        }\n",
            "    },\n",
            "    \"wikiqa\": {\n",
            "        \"hf_hub_url\": \"wiki_qa\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"question\",\n",
            "            \"response\": \"answer\"\n",
            "        }\n",
            "    },\n",
            "    \"webqa\": {\n",
            "        \"hf_hub_url\": \"suolyer/webqa\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/webqa\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"input\",\n",
            "            \"response\": \"output\"\n",
            "        }\n",
            "    },\n",
            "    \"webnovel\": {\n",
            "        \"hf_hub_url\": \"zxbsmk/webnovel_cn\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/webnovel_cn\"\n",
            "    },\n",
            "    \"nectar_sft\": {\n",
            "        \"hf_hub_url\": \"AstraMindAI/SFT-Nectar\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/SFT-Nectar\"\n",
            "    },\n",
            "    \"deepctrl\": {\n",
            "        \"ms_hub_url\": \"deepctrl/deepctrl-sft-data\"\n",
            "    },\n",
            "    \"adgen_train\": {\n",
            "        \"hf_hub_url\": \"HasturOfficial/adgen\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/adgen\",\n",
            "        \"split\": \"train\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"content\",\n",
            "            \"response\": \"summary\"\n",
            "        }\n",
            "    },\n",
            "    \"adgen_eval\": {\n",
            "        \"hf_hub_url\": \"HasturOfficial/adgen\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/adgen\",\n",
            "        \"split\": \"validation\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"content\",\n",
            "            \"response\": \"summary\"\n",
            "        }\n",
            "    },\n",
            "    \"sharegpt_hyper\": {\n",
            "        \"hf_hub_url\": \"totally-not-an-llm/sharegpt-hyperfiltered-3k\",\n",
            "        \"formatting\": \"sharegpt\"\n",
            "    },\n",
            "    \"sharegpt4\": {\n",
            "        \"hf_hub_url\": \"shibing624/sharegpt_gpt4\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/sharegpt_gpt4\",\n",
            "        \"formatting\": \"sharegpt\"\n",
            "    },\n",
            "    \"ultrachat_200k\": {\n",
            "        \"hf_hub_url\": \"HuggingFaceH4/ultrachat_200k\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/ultrachat_200k\",\n",
            "        \"split\": \"train_sft\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"messages\"\n",
            "        },\n",
            "        \"tags\": {\n",
            "            \"role_tag\": \"role\",\n",
            "            \"content_tag\": \"content\",\n",
            "            \"user_tag\": \"user\",\n",
            "            \"assistant_tag\": \"assistant\"\n",
            "        }\n",
            "    },\n",
            "    \"infinity_instruct\": {\n",
            "        \"hf_hub_url\": \"BAAI/Infinity-Instruct\",\n",
            "        \"formatting\": \"sharegpt\"\n",
            "    },\n",
            "    \"agent_instruct\": {\n",
            "        \"hf_hub_url\": \"THUDM/AgentInstruct\",\n",
            "        \"ms_hub_url\": \"ZhipuAI/AgentInstruct\",\n",
            "        \"formatting\": \"sharegpt\"\n",
            "    },\n",
            "    \"lmsys_chat\": {\n",
            "        \"hf_hub_url\": \"lmsys/lmsys-chat-1m\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/lmsys-chat-1m\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"conversation\"\n",
            "        },\n",
            "        \"tags\": {\n",
            "            \"role_tag\": \"role\",\n",
            "            \"content_tag\": \"content\",\n",
            "            \"user_tag\": \"user\",\n",
            "            \"assistant_tag\": \"assistant\"\n",
            "        }\n",
            "    },\n",
            "    \"evol_instruct\": {\n",
            "        \"hf_hub_url\": \"WizardLM/WizardLM_evol_instruct_V2_196k\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/WizardLM_evol_instruct_V2_196k\",\n",
            "        \"formatting\": \"sharegpt\"\n",
            "    },\n",
            "    \"glaive_toolcall_100k\": {\n",
            "        \"hf_hub_url\": \"hiyouga/glaive-function-calling-v2-sharegpt\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"conversations\",\n",
            "            \"tools\": \"tools\"\n",
            "        }\n",
            "    },\n",
            "    \"cosmopedia\": {\n",
            "        \"hf_hub_url\": \"HuggingFaceTB/cosmopedia\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"prompt\",\n",
            "            \"response\": \"text\"\n",
            "        }\n",
            "    },\n",
            "    \"stem_zh\": {\n",
            "        \"hf_hub_url\": \"hfl/stem_zh_instruction\"\n",
            "    },\n",
            "    \"ruozhiba_gpt4\": {\n",
            "        \"hf_hub_url\": \"hfl/ruozhiba_gpt4_turbo\"\n",
            "    },\n",
            "    \"neo_sft\": {\n",
            "        \"hf_hub_url\": \"m-a-p/neo_sft_phase2\",\n",
            "        \"formatting\": \"sharegpt\"\n",
            "    },\n",
            "    \"magpie_pro_300k\": {\n",
            "        \"hf_hub_url\": \"Magpie-Align/Magpie-Pro-300K-Filtered\",\n",
            "        \"formatting\": \"sharegpt\"\n",
            "    },\n",
            "    \"magpie_ultra\": {\n",
            "        \"hf_hub_url\": \"argilla/magpie-ultra-v0.1\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"instruction\",\n",
            "            \"response\": \"response\"\n",
            "        }\n",
            "    },\n",
            "    \"web_instruct\": {\n",
            "        \"hf_hub_url\": \"TIGER-Lab/WebInstructSub\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"question\",\n",
            "            \"response\": \"answer\"\n",
            "        }\n",
            "    },\n",
            "    \"openo1_sft\": {\n",
            "        \"hf_hub_url\": \"llamafactory/OpenO1-SFT\",\n",
            "        \"ms_hub_url\": \"llamafactory/OpenO1-SFT\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"prompt\",\n",
            "            \"response\": \"response\"\n",
            "        }\n",
            "    },\n",
            "    \"open_thoughts\": {\n",
            "        \"hf_hub_url\": \"llamafactory/OpenThoughts-114k\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"messages\"\n",
            "        },\n",
            "        \"tags\": {\n",
            "            \"role_tag\": \"role\",\n",
            "            \"content_tag\": \"content\",\n",
            "            \"user_tag\": \"user\",\n",
            "            \"assistant_tag\": \"assistant\",\n",
            "            \"system_tag\": \"system\"\n",
            "        }\n",
            "    },\n",
            "    \"open_r1_math\": {\n",
            "        \"hf_hub_url\": \"llamafactory/OpenR1-Math-94k\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"messages\"\n",
            "        },\n",
            "        \"tags\": {\n",
            "            \"role_tag\": \"role\",\n",
            "            \"content_tag\": \"content\",\n",
            "            \"user_tag\": \"user\",\n",
            "            \"assistant_tag\": \"assistant\",\n",
            "            \"system_tag\": \"system\"\n",
            "        }\n",
            "    },\n",
            "    \"chinese_r1_distill\": {\n",
            "        \"hf_hub_url\": \"Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT\",\n",
            "        \"ms_hub_url\": \"liucong/Chinese-DeepSeek-R1-Distill-data-110k-SFT\"\n",
            "    },\n",
            "    \"llava_1k_en\": {\n",
            "        \"hf_hub_url\": \"BUAADreamer/llava-en-zh-2k\",\n",
            "        \"subset\": \"en\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"messages\",\n",
            "            \"images\": \"images\"\n",
            "        },\n",
            "        \"tags\": {\n",
            "            \"role_tag\": \"role\",\n",
            "            \"content_tag\": \"content\",\n",
            "            \"user_tag\": \"user\",\n",
            "            \"assistant_tag\": \"assistant\"\n",
            "        }\n",
            "    },\n",
            "    \"llava_1k_zh\": {\n",
            "        \"hf_hub_url\": \"BUAADreamer/llava-en-zh-2k\",\n",
            "        \"subset\": \"zh\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"messages\",\n",
            "            \"images\": \"images\"\n",
            "        },\n",
            "        \"tags\": {\n",
            "            \"role_tag\": \"role\",\n",
            "            \"content_tag\": \"content\",\n",
            "            \"user_tag\": \"user\",\n",
            "            \"assistant_tag\": \"assistant\"\n",
            "        }\n",
            "    },\n",
            "    \"llava_150k_en\": {\n",
            "        \"hf_hub_url\": \"BUAADreamer/llava-en-zh-300k\",\n",
            "        \"subset\": \"en\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"messages\",\n",
            "            \"images\": \"images\"\n",
            "        },\n",
            "        \"tags\": {\n",
            "            \"role_tag\": \"role\",\n",
            "            \"content_tag\": \"content\",\n",
            "            \"user_tag\": \"user\",\n",
            "            \"assistant_tag\": \"assistant\"\n",
            "        }\n",
            "    },\n",
            "    \"llava_150k_zh\": {\n",
            "        \"hf_hub_url\": \"BUAADreamer/llava-en-zh-300k\",\n",
            "        \"subset\": \"zh\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"messages\",\n",
            "            \"images\": \"images\"\n",
            "        },\n",
            "        \"tags\": {\n",
            "            \"role_tag\": \"role\",\n",
            "            \"content_tag\": \"content\",\n",
            "            \"user_tag\": \"user\",\n",
            "            \"assistant_tag\": \"assistant\"\n",
            "        }\n",
            "    },\n",
            "    \"pokemon_cap\": {\n",
            "        \"hf_hub_url\": \"llamafactory/pokemon-gpt4o-captions\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"conversations\",\n",
            "            \"images\": \"images\"\n",
            "        }\n",
            "    },\n",
            "    \"mllm_pt_demo\": {\n",
            "        \"hf_hub_url\": \"BUAADreamer/mllm_pt_demo\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"messages\",\n",
            "            \"images\": \"images\"\n",
            "        },\n",
            "        \"tags\": {\n",
            "            \"role_tag\": \"role\",\n",
            "            \"content_tag\": \"content\",\n",
            "            \"user_tag\": \"user\",\n",
            "            \"assistant_tag\": \"assistant\"\n",
            "        }\n",
            "    },\n",
            "    \"oasst_de\": {\n",
            "        \"hf_hub_url\": \"mayflowergmbh/oasst_de\"\n",
            "    },\n",
            "    \"dolly_15k_de\": {\n",
            "        \"hf_hub_url\": \"mayflowergmbh/dolly-15k_de\"\n",
            "    },\n",
            "    \"alpaca-gpt4_de\": {\n",
            "        \"hf_hub_url\": \"mayflowergmbh/alpaca-gpt4_de\"\n",
            "    },\n",
            "    \"openschnabeltier_de\": {\n",
            "        \"hf_hub_url\": \"mayflowergmbh/openschnabeltier_de\"\n",
            "    },\n",
            "    \"evol_instruct_de\": {\n",
            "        \"hf_hub_url\": \"mayflowergmbh/evol-instruct_de\"\n",
            "    },\n",
            "    \"dolphin_de\": {\n",
            "        \"hf_hub_url\": \"mayflowergmbh/dolphin_de\"\n",
            "    },\n",
            "    \"booksum_de\": {\n",
            "        \"hf_hub_url\": \"mayflowergmbh/booksum_de\"\n",
            "    },\n",
            "    \"airoboros_de\": {\n",
            "        \"hf_hub_url\": \"mayflowergmbh/airoboros-3.0_de\"\n",
            "    },\n",
            "    \"ultrachat_de\": {\n",
            "        \"hf_hub_url\": \"mayflowergmbh/ultra-chat_de\"\n",
            "    },\n",
            "    \"dpo_en_demo\": {\n",
            "        \"file_name\": \"dpo_en_demo.json\",\n",
            "        \"ranking\": true,\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"conversations\",\n",
            "            \"chosen\": \"chosen\",\n",
            "            \"rejected\": \"rejected\"\n",
            "        }\n",
            "    },\n",
            "    \"dpo_zh_demo\": {\n",
            "        \"file_name\": \"dpo_zh_demo.json\",\n",
            "        \"ranking\": true,\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"conversations\",\n",
            "            \"chosen\": \"chosen\",\n",
            "            \"rejected\": \"rejected\"\n",
            "        }\n",
            "    },\n",
            "    \"dpo_mix_en\": {\n",
            "        \"hf_hub_url\": \"llamafactory/DPO-En-Zh-20k\",\n",
            "        \"subset\": \"en\",\n",
            "        \"ranking\": true,\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"conversations\",\n",
            "            \"chosen\": \"chosen\",\n",
            "            \"rejected\": \"rejected\"\n",
            "        }\n",
            "    },\n",
            "    \"dpo_mix_zh\": {\n",
            "        \"hf_hub_url\": \"llamafactory/DPO-En-Zh-20k\",\n",
            "        \"subset\": \"zh\",\n",
            "        \"ranking\": true,\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"conversations\",\n",
            "            \"chosen\": \"chosen\",\n",
            "            \"rejected\": \"rejected\"\n",
            "        }\n",
            "    },\n",
            "    \"ultrafeedback\": {\n",
            "        \"hf_hub_url\": \"llamafactory/ultrafeedback_binarized\",\n",
            "        \"ms_hub_url\": \"llamafactory/ultrafeedback_binarized\",\n",
            "        \"ranking\": true,\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"instruction\",\n",
            "            \"chosen\": \"chosen\",\n",
            "            \"rejected\": \"rejected\"\n",
            "        }\n",
            "    },\n",
            "    \"coig_p\": {\n",
            "        \"hf_hub_url\": \"m-a-p/COIG-P\",\n",
            "        \"ranking\": true,\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"conversations\",\n",
            "            \"chosen\": \"chosen\",\n",
            "            \"rejected\": \"rejected\"\n",
            "        }\n",
            "    },\n",
            "    \"rlhf_v\": {\n",
            "        \"hf_hub_url\": \"llamafactory/RLHF-V\",\n",
            "        \"ranking\": true,\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"conversations\",\n",
            "            \"chosen\": \"chosen\",\n",
            "            \"rejected\": \"rejected\",\n",
            "            \"images\": \"images\"\n",
            "        }\n",
            "    },\n",
            "    \"vlfeedback\": {\n",
            "        \"hf_hub_url\": \"Zhihui/VLFeedback\",\n",
            "        \"ranking\": true,\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"conversations\",\n",
            "            \"chosen\": \"chosen\",\n",
            "            \"rejected\": \"rejected\",\n",
            "            \"images\": \"images\"\n",
            "        }\n",
            "    },\n",
            "    \"rlaif_v\": {\n",
            "        \"hf_hub_url\": \"openbmb/RLAIF-V-Dataset\",\n",
            "        \"ranking\": true,\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"question\",\n",
            "            \"chosen\": \"chosen\",\n",
            "            \"rejected\": \"rejected\",\n",
            "            \"images\": \"image\"\n",
            "        }\n",
            "    },\n",
            "    \"orca_pairs\": {\n",
            "        \"hf_hub_url\": \"Intel/orca_dpo_pairs\",\n",
            "        \"ranking\": true,\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"question\",\n",
            "            \"chosen\": \"chosen\",\n",
            "            \"rejected\": \"rejected\",\n",
            "            \"system\": \"system\"\n",
            "        }\n",
            "    },\n",
            "    \"nectar_rm\": {\n",
            "        \"hf_hub_url\": \"AstraMindAI/RLAIF-Nectar\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/RLAIF-Nectar\",\n",
            "        \"ranking\": true\n",
            "    },\n",
            "    \"orca_dpo_de\": {\n",
            "        \"hf_hub_url\": \"mayflowergmbh/intel_orca_dpo_pairs_de\",\n",
            "        \"ranking\": true\n",
            "    },\n",
            "    \"kto_en_demo\": {\n",
            "        \"file_name\": \"kto_en_demo.json\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"messages\",\n",
            "            \"kto_tag\": \"label\"\n",
            "        },\n",
            "        \"tags\": {\n",
            "            \"role_tag\": \"role\",\n",
            "            \"content_tag\": \"content\",\n",
            "            \"user_tag\": \"user\",\n",
            "            \"assistant_tag\": \"assistant\"\n",
            "        }\n",
            "    },\n",
            "    \"kto_mix_en\": {\n",
            "        \"hf_hub_url\": \"argilla/kto-mix-15k\",\n",
            "        \"formatting\": \"sharegpt\",\n",
            "        \"columns\": {\n",
            "            \"messages\": \"completion\",\n",
            "            \"kto_tag\": \"label\"\n",
            "        },\n",
            "        \"tags\": {\n",
            "            \"role_tag\": \"role\",\n",
            "            \"content_tag\": \"content\",\n",
            "            \"user_tag\": \"user\",\n",
            "            \"assistant_tag\": \"assistant\"\n",
            "        }\n",
            "    },\n",
            "    \"ultrafeedback_kto\": {\n",
            "        \"hf_hub_url\": \"argilla/ultrafeedback-binarized-preferences-cleaned-kto\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/ultrafeedback-binarized-preferences-cleaned-kto\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"prompt\",\n",
            "            \"response\": \"completion\",\n",
            "            \"kto_tag\": \"label\"\n",
            "        }\n",
            "    },\n",
            "    \"wiki_demo\": {\n",
            "        \"file_name\": \"wiki_demo.txt\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"text\"\n",
            "        }\n",
            "    },\n",
            "    \"c4_demo\": {\n",
            "        \"file_name\": \"c4_demo.jsonl\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"text\"\n",
            "        }\n",
            "    },\n",
            "    \"refinedweb\": {\n",
            "        \"hf_hub_url\": \"tiiuae/falcon-refinedweb\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"content\"\n",
            "        }\n",
            "    },\n",
            "    \"redpajama_v2\": {\n",
            "        \"hf_hub_url\": \"togethercomputer/RedPajama-Data-V2\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"raw_content\"\n",
            "        },\n",
            "        \"subset\": \"default\"\n",
            "    },\n",
            "    \"wikipedia_en\": {\n",
            "        \"hf_hub_url\": \"olm/olm-wikipedia-20221220\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/olm-wikipedia-20221220\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"text\"\n",
            "        }\n",
            "    },\n",
            "    \"wikipedia_zh\": {\n",
            "        \"hf_hub_url\": \"pleisto/wikipedia-cn-20230720-filtered\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/wikipedia-cn-20230720-filtered\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"completion\"\n",
            "        }\n",
            "    },\n",
            "    \"pile\": {\n",
            "        \"hf_hub_url\": \"monology/pile-uncopyrighted\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/pile\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"text\"\n",
            "        }\n",
            "    },\n",
            "    \"skypile\": {\n",
            "        \"hf_hub_url\": \"Skywork/SkyPile-150B\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/SkyPile-150B\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"text\"\n",
            "        }\n",
            "    },\n",
            "    \"fineweb\": {\n",
            "        \"hf_hub_url\": \"HuggingFaceFW/fineweb\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"text\"\n",
            "        }\n",
            "    },\n",
            "    \"fineweb_edu\": {\n",
            "        \"hf_hub_url\": \"HuggingFaceFW/fineweb-edu\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"text\"\n",
            "        }\n",
            "    },\n",
            "    \"cci3_hq\": {\n",
            "        \"hf_hub_url\": \"BAAI/CCI3-HQ\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"text\"\n",
            "        }\n",
            "    },\n",
            "    \"cci3_data\": {\n",
            "        \"hf_hub_url\": \"BAAI/CCI3-Data\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"text\"\n",
            "        }\n",
            "    },\n",
            "    \"cci4_base\": {\n",
            "        \"hf_hub_url\": \"BAAI/CCI4.0-M2-Base-v1\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"text\"\n",
            "        }\n",
            "    },\n",
            "    \"cci4_cot\": {\n",
            "        \"hf_hub_url\": \"BAAI/CCI4.0-M2-CoT-v1\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"text\"\n",
            "        }\n",
            "    },\n",
            "    \"cci4_extra\": {\n",
            "        \"hf_hub_url\": \"BAAI/CCI4.0-M2-Extra-v1\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"text\"\n",
            "        }\n",
            "    },\n",
            "    \"the_stack\": {\n",
            "        \"hf_hub_url\": \"bigcode/the-stack\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/the-stack\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"content\"\n",
            "        }\n",
            "    },\n",
            "    \"starcoder_python\": {\n",
            "        \"hf_hub_url\": \"bigcode/starcoderdata\",\n",
            "        \"ms_hub_url\": \"AI-ModelScope/starcoderdata\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"content\"\n",
            "        },\n",
            "        \"folder\": \"python\"\n",
            "    },\n",
            "    \"code_finetune_train\": {\n",
            "        \"file_name\": \"/kaggle/working/datasets/llamafactory-finetune-data/train.jsonl\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"instruction\",\n",
            "            \"query\": \"input\",\n",
            "            \"response\": \"output\",\n",
            "            \"system\": \"system\",\n",
            "            \"history\": \"history\"\n",
            "        }\n",
            "    },\n",
            "    \"code_finetune_val\": {\n",
            "        \"file_name\": \"/kaggle/working/datasets/llamafactory-finetune-data/val.jsonl\",\n",
            "        \"columns\": {\n",
            "            \"prompt\": \"instruction\",\n",
            "            \"query\": \"input\",\n",
            "            \"response\": \"output\",\n",
            "            \"system\": \"system\",\n",
            "            \"history\": \"history\"\n",
            "        }\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "with open(json_path) as f:\n",
        "    print(json.dumps(json.load(f), indent=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:27:59.160555Z",
          "iopub.status.busy": "2025-11-28T15:27:59.160277Z",
          "iopub.status.idle": "2025-11-28T15:27:59.16504Z",
          "shell.execute_reply": "2025-11-28T15:27:59.164404Z",
          "shell.execute_reply.started": "2025-11-28T15:27:59.160534Z"
        },
        "id": "NNWXaqfUM9Og",
        "outputId": "579550b2-f0cb-4b90-9010-35e0403a45cb",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder 'models' created!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"/kaggle/working/models\", exist_ok=True)\n",
        "print(\"Folder 'models' created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:28:04.490311Z",
          "iopub.status.busy": "2025-11-28T15:28:04.489739Z",
          "iopub.status.idle": "2025-11-28T15:28:04.495777Z",
          "shell.execute_reply": "2025-11-28T15:28:04.495177Z",
          "shell.execute_reply.started": "2025-11-28T15:28:04.490285Z"
        },
        "id": "rdcAufEhM9Og",
        "outputId": "b57b9f7f-0a7a-45dc-f2a9-ced798f24962",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /kaggle/working/LLaMA-Factory/examples/train_lora/code_finetune.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile /kaggle/working/LLaMA-Factory/examples/train_lora/code_finetune.yaml\n",
        "\n",
        "### model\n",
        "model_name_or_path: Qwen/Qwen2.5-3B-Instruct\n",
        "trust_remote_code: true\n",
        "\n",
        "### method\n",
        "stage: sft\n",
        "do_train: true\n",
        "finetuning_type: lora\n",
        "lora_rank: 64\n",
        "lora_target: all\n",
        "\n",
        "### dataset\n",
        "dataset: code_finetune_train\n",
        "eval_dataset: code_finetune_val\n",
        "template: qwen\n",
        "cutoff_len: 3500\n",
        "overwrite_cache: true\n",
        "preprocessing_num_workers: 8\n",
        "\n",
        "### output\n",
        "output_dir: /kaggle/working/models\n",
        "logging_steps: 10\n",
        "save_steps: 1000\n",
        "plot_loss: true\n",
        "\n",
        "### train\n",
        "per_device_train_batch_size: 4\n",
        "gradient_accumulation_steps: 4\n",
        "learning_rate: 1.0e-4\n",
        "num_train_epochs: 2.0\n",
        "lr_scheduler_type: cosine\n",
        "warmup_ratio: 0.1\n",
        "bf16: true\n",
        "ddp_timeout: 180000000\n",
        "\n",
        "### eval\n",
        "per_device_eval_batch_size: 1\n",
        "eval_strategy: steps\n",
        "eval_steps: 1000\n",
        "\n",
        "report_to: wandb\n",
        "run_name: code-finetune-llamafactory\n",
        "\n",
        "push_to_hub: true\n",
        "hub_model_id: \"abdelalem33/coder\"\n",
        "hub_private_repo: true\n",
        "hub_strategy: checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T15:28:09.848633Z",
          "iopub.status.busy": "2025-11-28T15:28:09.847899Z",
          "iopub.status.idle": "2025-11-28T18:56:59.653737Z",
          "shell.execute_reply": "2025-11-28T18:56:59.652923Z",
          "shell.execute_reply.started": "2025-11-28T15:28:09.848611Z"
        },
        "id": "eubeyZCzM9Og",
        "outputId": "b71fe874-795e-437f-fefe-132045578242",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-28 15:28:16.969566: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764343697.146377     302 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764343697.196160     302 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "[INFO|2025-11-28 15:28:43] llamafactory.hparams.parser:468 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
            "tokenizer_config.json: 7.30kB [00:00, 24.8MB/s]\n",
            "vocab.json: 2.78MB [00:00, 12.2MB/s]\n",
            "merges.txt: 1.67MB [00:00, 8.21MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 20.9MB/s]\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-11-28 15:28:46,220 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-11-28 15:28:46,220 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-11-28 15:28:46,220 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-11-28 15:28:46,220 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-11-28 15:28:46,220 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-11-28 15:28:46,220 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-11-28 15:28:46,221 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2364] 2025-11-28 15:28:46,522 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "config.json: 100%|█████████████████████████████| 661/661 [00:00<00:00, 5.14MB/s]\n",
            "[INFO|configuration_utils.py:765] 2025-11-28 15:28:53,176 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-11-28 15:28:53,178 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 70,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-11-28 15:28:53,375 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-11-28 15:28:53,375 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-11-28 15:28:53,375 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-11-28 15:28:53,376 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-11-28 15:28:53,376 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-11-28 15:28:53,376 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-11-28 15:28:53,376 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2364] 2025-11-28 15:28:53,695 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-11-28 15:28:53] llamafactory.data.loader:143 >> Loading dataset /kaggle/working/datasets/llamafactory-finetune-data/train.jsonl...\n",
            "Setting num_proc from 8 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "WARNING:datasets.builder:Setting num_proc from 8 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 4000 examples [00:00, 96857.20 examples/s]\n",
            "Converting format of dataset (num_proc=8): 100%|█| 4000/4000 [00:00<00:00, 6562.\n",
            "[INFO|2025-11-28 15:28:54] llamafactory.data.loader:143 >> Loading dataset /kaggle/working/datasets/llamafactory-finetune-data/val.jsonl...\n",
            "Setting num_proc from 8 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "WARNING:datasets.builder:Setting num_proc from 8 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 758 examples [00:00, 110288.36 examples/s]\n",
            "Converting format of dataset (num_proc=8): 100%|█| 758/758 [00:00<00:00, 1855.97\n",
            "Running tokenizer on dataset (num_proc=8): 100%|█| 4000/4000 [00:04<00:00, 834.4\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 458, 6203, 13027, 2038, 13823, 624, 2610, 686, 5258, 264, 10916, 25128, 22692, 264, 15473, 3383, 624, 31115, 4240, 11, 11050, 13027, 2038, 429, 6896, 5169, 279, 25128, 624, 12480, 1493, 5601, 510, 16, 13, 5443, 279, 4734, 3890, 5036, 5189, 304, 279, 1946, 198, 17, 13, 14152, 279, 821, 14389, 323, 4494, 9733, 198, 18, 13, 31075, 279, 23560, 5666, 7481, 198, 19, 13, 19813, 1172, 279, 2038, 7493, 476, 5114, 481, 902, 40841, 11, 902, 6042, 11, 902, 50494, 198, 20, 13, 9645, 63594, 11, 13027, 292, 2038, 1667, 8311, 5798, 3419, 5746, 323, 1140, 12674, 4664, 198, 21, 13, 13760, 279, 4734, 1946, 47016, 3561, 5189, 198, 5404, 537, 912, 894, 16800, 11, 16148, 11, 476, 16688, 624, 5097, 1172, 32156, 13027, 2038, 13, 151645, 198, 151644, 872, 198, 2, 11536, 510, 4340, 646, 358, 1779, 421, 264, 31300, 374, 10067, 304, 85717, 13027, 4895, 12521, 1939, 2, 6262, 510, 4934, 279, 4362, 2038, 304, 10135, 1119, 264, 4718, 382, 2, 5097, 43781, 510, 4913, 13193, 788, 5212, 2078, 788, 5212, 4684, 788, 330, 785, 9258, 2038, 304, 13027, 10465, 330, 60992, 788, 220, 18, 15, 15, 11, 330, 1065, 4373, 788, 220, 20, 11, 330, 2102, 788, 330, 2078, 497, 330, 1313, 788, 330, 917, 9207, 2137, 330, 6279, 788, 4383, 2078, 7914, 330, 2102, 788, 330, 2078, 497, 330, 1313, 788, 330, 1700, 63159, 2, 5097, 8308, 510, 73594, 2236, 151645, 198, 151644, 77091, 198, 73594, 2236, 198, 1, 12521, 2658, 7894, 3710, 1269, 11146, 2028, 10194, 1269, 57576, 285, 23755, 99144, 73594, 151645, 198]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are an expert Python code generator.\n",
            "You will receive a technical specification describing a programming task.\n",
            "Generate clean, efficient Python code that exactly implements the specification.\n",
            "Follow these rules:\n",
            "1. Use the exact variable names specified in the input\n",
            "2. Match the data structures and types mentioned\n",
            "3. Implement the precise operation described\n",
            "4. Generate only the code expression or statement - no explanations, no comments, no markdown\n",
            "5. Write concise, Pythonic code using appropriate built-in functions and list comprehensions\n",
            "6. Handle the exact input/output format specified\n",
            "Do not add any introduction, explanation, or conclusion.\n",
            "Output only executable Python code.<|im_end|>\n",
            "<|im_start|>user\n",
            "#Intent:\n",
            "How can I check if a checkbox is checked in Selenium Python Webdriver?\n",
            "\n",
            "#Task:\n",
            "write the needed code in python into a JSON.\n",
            "\n",
            "#Output Scheme:\n",
            "{\"properties\": {\"Code\": {\"description\": \"The Output code in Python.\", \"maxLength\": 300, \"minLength\": 5, \"title\": \"Code\", \"type\": \"string\"}}, \"required\": [\"Code\"], \"title\": \"Code\", \"type\": \"object\"}\n",
            "\n",
            "#Output Json:\n",
            "```json<|im_end|>\n",
            "<|im_start|>assistant\n",
            "```json\n",
            "\"driver.find_element_by_name('<check_box_name>').is_selected()\"\n",
            "```<|im_end|>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 73594, 2236, 198, 1, 12521, 2658, 7894, 3710, 1269, 11146, 2028, 10194, 1269, 57576, 285, 23755, 99144, 73594, 151645, 198]\n",
            "labels:\n",
            "```json\n",
            "\"driver.find_element_by_name('<check_box_name>').is_selected()\"\n",
            "```<|im_end|>\n",
            "\n",
            "Running tokenizer on dataset (num_proc=8): 100%|█| 758/758 [00:02<00:00, 255.22 \n",
            "eval example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10916, 7385, 312, 18189, 20045, 28703, 369, 444, 10994, 2038, 9471, 624, 7771, 58524, 7385, 1969, 3410, 13097, 1995, 4362, 369, 458, 444, 10994, 311, 6923, 4396, 2038, 6007, 5107, 2266, 624, 37438, 2283, 5424, 311, 2924, 510, 16, 13, 12407, 5036, 481, 20678, 11464, 5036, 320, 87, 11, 274, 11, 294, 11, 18845, 8, 773, 279, 444, 10994, 8788, 1128, 311, 990, 198, 17, 13, 2885, 14389, 481, 47395, 1140, 11, 6451, 11, 738, 11, 914, 11, 14405, 11, 4992, 624, 18, 13, 2885, 4494, 481, 3234, 25780, 11, 9069, 11, 47902, 11, 708, 96999, 11, 4992, 624, 19, 13, 16730, 481, 5443, 23560, 61846, 25, 77784, 11, 4051, 11, 5165, 11, 10880, 11, 8649, 11, 23192, 11, 3378, 198, 20, 13, 9258, 3561, 481, 60785, 1128, 279, 1102, 1265, 387, 198, 21, 13, 97032, 3565, 481, 7217, 45543, 11, 4287, 1142, 11589, 11, 943, 48722, 198, 17082, 25, 364, 63136, 5109, 304, 264, 1140, 1248, 15216, 25, 364, 78440, 31123, 5424, 315, 264, 1140, 364, 87, 6, 315, 5248, 25780, 311, 264, 3175, 7546, 1248, 10234, 30, 576, 1661, 2319, 10742, 279, 444, 10994, 510, 12, 12407, 829, 25, 364, 87, 1248, 12, 5571, 943, 25, 1140, 315, 25780, 198, 12, 16730, 25, 77784, 320, 1921, 2629, 11, 537, 5138, 448, 29020, 340, 12, 9258, 943, 25, 3175, 7546, 151645, 198, 151644, 872, 198, 2, 11536, 510, 19366, 12782, 504, 50494, 1034, 271, 2, 6262, 510, 4934, 279, 23922, 2319, 315, 279, 7385, 1119, 264, 4718, 382, 2, 5097, 43781, 510, 4913, 13193, 788, 5212, 39867, 4874, 94249, 788, 5212, 4684, 788, 330, 785, 23922, 2319, 315, 8829, 10465, 330, 60992, 788, 220, 18, 15, 15, 11, 330, 1065, 4373, 788, 220, 20, 11, 330, 2102, 788, 330, 57468, 4874, 8829, 497, 330, 1313, 788, 330, 917, 9207, 2137, 330, 6279, 788, 4383, 39867, 4874, 94249, 7914, 330, 2102, 788, 330, 11536, 497, 330, 1313, 788, 330, 1700, 63159, 2, 5097, 8308, 510, 73594, 2236, 151645, 198, 151644, 77091, 198, 73594, 2236, 198, 1, 19366, 12782, 1034, 1565, 3006, 13323, 63, 504, 50494, 1034, 1565, 1355, 13323, 63, 698, 73594, 151645, 198]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a technical intent rewriter preparing specifications for LLM code generation.\n",
            "Your rewritten intent must provide ALL information needed for an LLM to generate correct code WITHOUT additional context.\n",
            "Essential elements to include:\n",
            "1. Variable names - Give explicit names (x, s, d, lst) so the LLM knows what to use\n",
            "2. Data structures - Specify list, dict, set, string, tuple, etc.\n",
            "3. Data types - State integers, strings, floats, booleans, etc.\n",
            "4. Operation - Use precise verbs: concatenate, filter, transform, merge, extract, aggregate, sort\n",
            "5. Output format - Describe what the result should be\n",
            "6. Implicit details - Order preservation, empty case handling, type conversions\n",
            "Bad: 'combine numbers in a list'\n",
            "Good: 'Concatenate elements of a list 'x' of multiple integers to a single integer'\n",
            "Why? The good version tells the LLM:\n",
            "- Variable name: 'x'\n",
            "- Input type: list of integers\n",
            "- Operation: concatenate (not sum, not join with delimiter)\n",
            "- Output type: single integer<|im_end|>\n",
            "<|im_start|>user\n",
            "#Intent:\n",
            "generate pdf from markdown file\n",
            "\n",
            "#Task:\n",
            "write the enhanced version of the intent into a JSON.\n",
            "\n",
            "#Output Scheme:\n",
            "{\"properties\": {\"enhanced_intent\": {\"description\": \"The enhanced version of Intent.\", \"maxLength\": 300, \"minLength\": 5, \"title\": \"Enhanced Intent\", \"type\": \"string\"}}, \"required\": [\"enhanced_intent\"], \"title\": \"Intent\", \"type\": \"object\"}\n",
            "\n",
            "#Output Json:\n",
            "```json<|im_end|>\n",
            "<|im_start|>assistant\n",
            "```json\n",
            "\"generate pdf file `output_filename` from markdown file `input_filename`\"\n",
            "```<|im_end|>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 73594, 2236, 198, 1, 19366, 12782, 1034, 1565, 3006, 13323, 63, 504, 50494, 1034, 1565, 1355, 13323, 63, 698, 73594, 151645, 198]\n",
            "labels:\n",
            "```json\n",
            "\"generate pdf file `output_filename` from markdown file `input_filename`\"\n",
            "```<|im_end|>\n",
            "\n",
            "[INFO|configuration_utils.py:765] 2025-11-28 15:29:03,348 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-11-28 15:29:03,349 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 70,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|2025-11-28 15:29:03] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "[WARNING|logging.py:328] 2025-11-28 15:29:04,979 >> `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors.index.json: 35.6kB [00:00, 27.4MB/s]\n",
            "[INFO|modeling_utils.py:1172] 2025-11-28 15:29:05,367 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/model.safetensors.index.json\n",
            "Fetching 2 files:   0%|                                   | 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0%|             | 0.00/3.97G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   0%|             | 0.00/2.20G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   0%|      | 866k/2.20G [00:01<46:10, 795kB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   0%| | 20.3k/3.97G [00:01<75:32:48, 14.6kB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1%|    | 33.1M/3.97G [00:01<02:30, 26.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   1%|    | 24.2M/2.20G [00:01<02:08, 16.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   2%|    | 38.8M/2.20G [00:01<01:15, 28.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   1%|    | 44.9M/3.97G [00:01<02:11, 29.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   3%|▏   | 74.6M/2.20G [00:02<00:34, 62.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   2%|    | 61.0M/3.97G [00:02<01:42, 38.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   4%|▏   | 89.5M/2.20G [00:02<00:35, 59.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   4%|▏     | 157M/3.97G [00:02<00:26, 143MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   6%|▎    | 124M/2.20G [00:02<00:25, 81.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   5%|▏    | 196M/3.97G [00:03<00:38, 98.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   7%|▎    | 149M/2.20G [00:03<00:29, 68.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  10%|▌     | 222M/2.20G [00:03<00:14, 137MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   7%|▍     | 281M/3.97G [00:03<00:29, 127MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  13%|▊     | 289M/2.20G [00:03<00:14, 129MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   8%|▍     | 303M/3.97G [00:03<00:33, 111MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  15%|▉     | 328M/2.20G [00:04<00:14, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  17%|█     | 375M/2.20G [00:04<00:10, 166MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  10%|▌     | 380M/3.97G [00:04<00:30, 119MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11%|▋     | 447M/3.97G [00:04<00:24, 143MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  20%|█▏    | 442M/2.20G [00:04<00:13, 135MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  13%|▊     | 514M/3.97G [00:04<00:18, 187MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  23%|█▍    | 509M/2.20G [00:04<00:09, 180MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  15%|▉     | 608M/3.97G [00:05<00:13, 252MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  30%|█▊    | 664M/2.20G [00:05<00:04, 329MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  17%|█     | 678M/3.97G [00:05<00:13, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19%|█▏    | 745M/3.97G [00:05<00:12, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20%|█▏    | 812M/3.97G [00:05<00:12, 259MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  34%|██    | 747M/2.20G [00:05<00:07, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  38%|██▎   | 832M/2.20G [00:06<00:05, 252MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  22%|█▎    | 870M/3.97G [00:06<00:11, 263MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  40%|██▍   | 886M/2.20G [00:06<00:06, 217MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  42%|██▌   | 934M/2.20G [00:06<00:05, 222MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  24%|█▍    | 938M/3.97G [00:06<00:16, 184MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29%|█▍   | 1.14G/3.97G [00:06<00:07, 361MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  46%|██▎  | 1.01G/2.20G [00:06<00:05, 219MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  31%|█▌   | 1.21G/3.97G [00:07<00:09, 301MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  48%|██▍  | 1.05G/2.20G [00:07<00:05, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  53%|██▋  | 1.17G/2.20G [00:07<00:04, 254MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  32%|█▌   | 1.29G/3.97G [00:07<00:11, 226MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33%|█▋   | 1.33G/3.97G [00:07<00:11, 225MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  56%|██▊  | 1.23G/2.20G [00:08<00:04, 198MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  35%|█▋   | 1.39G/3.97G [00:08<00:10, 239MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  60%|██▉  | 1.32G/2.20G [00:08<00:04, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  65%|███▏ | 1.43G/2.20G [00:08<00:02, 285MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  36%|█▊   | 1.44G/3.97G [00:08<00:14, 172MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38%|█▉   | 1.49G/3.97G [00:09<00:16, 153MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38%|█▉   | 1.52G/3.97G [00:09<00:17, 139MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  68%|███▍ | 1.50G/2.20G [00:09<00:04, 164MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  39%|█▉   | 1.54G/3.97G [00:09<00:18, 134MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40%|██   | 1.59G/3.97G [00:09<00:14, 165MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  71%|███▌ | 1.57G/2.20G [00:10<00:04, 155MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  41%|██   | 1.62G/3.97G [00:10<00:16, 141MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42%|██   | 1.66G/3.97G [00:10<00:13, 167MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44%|██▏  | 1.74G/3.97G [00:10<00:09, 235MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46%|██▎  | 1.82G/3.97G [00:10<00:09, 222MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  74%|███▋ | 1.63G/2.20G [00:10<00:04, 116MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  47%|██▎  | 1.85G/3.97G [00:11<00:10, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48%|██▍  | 1.89G/3.97G [00:11<00:10, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50%|██▍  | 1.97G/3.97G [00:11<00:07, 264MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51%|██▌  | 2.03G/3.97G [00:11<00:07, 253MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53%|██▋  | 2.09G/3.97G [00:11<00:06, 290MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56%|██▊  | 2.22G/3.97G [00:11<00:03, 450MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  78%|███▉ | 1.72G/2.20G [00:12<00:04, 101MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  57%|██▊  | 2.28G/3.97G [00:12<00:04, 338MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59%|██▉  | 2.35G/3.97G [00:12<00:06, 234MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62%|███  | 2.44G/3.97G [00:13<00:05, 268MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  81%|███▎| 1.79G/2.20G [00:13<00:05, 79.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  84%|███▍| 1.86G/2.20G [00:14<00:04, 78.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  87%|███▍| 1.91G/2.20G [00:15<00:04, 63.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  89%|███▌| 1.96G/2.20G [00:19<00:07, 33.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  64%|██▌ | 2.54G/3.97G [00:19<00:35, 40.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68%|██▋ | 2.69G/3.97G [00:19<00:18, 70.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71%|██▊ | 2.80G/3.97G [00:20<00:15, 73.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72%|██▉ | 2.87G/3.97G [00:20<00:12, 90.0MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  92%|███▋| 2.02G/2.20G [00:20<00:05, 35.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  94%|███▊| 2.07G/2.20G [00:22<00:03, 36.0MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  75%|███ | 2.98G/3.97G [00:22<00:11, 84.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77%|███ | 3.05G/3.97G [00:23<00:11, 83.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78%|███ | 3.09G/3.97G [00:23<00:09, 91.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors: 100%|████| 2.20G/2.20G [00:23<00:00, 94.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  79%|███▉ | 3.15G/3.97G [00:23<00:06, 120MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83%|████▏| 3.29G/3.97G [00:23<00:03, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86%|████▎| 3.41G/3.97G [00:23<00:02, 273MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90%|████▌| 3.57G/3.97G [00:23<00:00, 401MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93%|████▋| 3.70G/3.97G [00:24<00:00, 465MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100%|█████| 3.97G/3.97G [00:24<00:00, 164MB/s]\u001b[A\n",
            "Fetching 2 files: 100%|███████████████████████████| 2/2 [00:24<00:00, 12.28s/it]\n",
            "[INFO|modeling_utils.py:2341] 2025-11-28 15:29:30,026 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:986] 2025-11-28 15:29:30,028 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.72s/it]\n",
            "generation_config.json: 100%|██████████████████| 242/242 [00:00<00:00, 1.88MB/s]\n",
            "[INFO|configuration_utils.py:941] 2025-11-28 15:29:33,835 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/generation_config.json\n",
            "[INFO|configuration_utils.py:986] 2025-11-28 15:29:33,835 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.05,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|dynamic_module_utils.py:423] 2025-11-28 15:29:33,923 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-3B-Instruct.\n",
            "[INFO|2025-11-28 15:29:33] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-11-28 15:29:33] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-11-28 15:29:33] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-11-28 15:29:33] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-11-28 15:29:33] llamafactory.model.model_utils.misc:143 >> Found linear modules: gate_proj,o_proj,k_proj,q_proj,up_proj,v_proj,down_proj\n",
            "[INFO|2025-11-28 15:29:40] llamafactory.model.loader:143 >> trainable params: 119,734,272 || all params: 3,205,672,960 || trainable%: 3.7351\n",
            "[WARNING|trainer.py:906] 2025-11-28 15:29:40,236 >> The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "[INFO|trainer.py:749] 2025-11-28 15:29:40,357 >> Using auto half precision backend\n",
            "[WARNING|trainer.py:982] 2025-11-28 15:29:40,359 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "[INFO|trainer.py:2519] 2025-11-28 15:29:41,013 >> ***** Running training *****\n",
            "[INFO|trainer.py:2520] 2025-11-28 15:29:41,013 >>   Num examples = 4,000\n",
            "[INFO|trainer.py:2521] 2025-11-28 15:29:41,013 >>   Num Epochs = 2\n",
            "[INFO|trainer.py:2522] 2025-11-28 15:29:41,013 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:2525] 2025-11-28 15:29:41,013 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2526] 2025-11-28 15:29:41,013 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2527] 2025-11-28 15:29:41,013 >>   Total optimization steps = 500\n",
            "[INFO|trainer.py:2528] 2025-11-28 15:29:41,018 >>   Number of trainable parameters = 119,734,272\n",
            "[INFO|integration_utils.py:867] 2025-11-28 15:29:41,024 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdelalem999\u001b[0m (\u001b[33mabdelalem999-org\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/LLaMA-Factory/wandb/run-20251128_152941-3rhyeotg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcode-finetune-llamafactory\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/abdelalem999-org/llamafactory\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/abdelalem999-org/llamafactory/runs/3rhyeotg\u001b[0m\n",
            "{'loss': 3.4786, 'grad_norm': 10.918046951293945, 'learning_rate': 1.8e-05, 'epoch': 0.04}\n",
            "{'loss': 1.5051, 'grad_norm': 1.9873095750808716, 'learning_rate': 3.8e-05, 'epoch': 0.08}\n",
            "{'loss': 1.2661, 'grad_norm': 1.7800116539001465, 'learning_rate': 5.8e-05, 'epoch': 0.12}\n",
            "{'loss': 1.1178, 'grad_norm': 1.4003158807754517, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.16}\n",
            "{'loss': 1.0445, 'grad_norm': 1.434427261352539, 'learning_rate': 9.8e-05, 'epoch': 0.2}\n",
            "{'loss': 1.0906, 'grad_norm': 1.15341055393219, 'learning_rate': 9.990133642141359e-05, 'epoch': 0.24}\n",
            "{'loss': 1.0894, 'grad_norm': 1.3361924886703491, 'learning_rate': 9.956077701257709e-05, 'epoch': 0.28}\n",
            "{'loss': 0.9745, 'grad_norm': 1.3050892353057861, 'learning_rate': 9.89787624799672e-05, 'epoch': 0.32}\n",
            "{'loss': 1.0677, 'grad_norm': 1.012281894683838, 'learning_rate': 9.815812833988291e-05, 'epoch': 0.36}\n",
            "{'loss': 0.9956, 'grad_norm': 1.1194453239440918, 'learning_rate': 9.710287263936484e-05, 'epoch': 0.4}\n",
            "{'loss': 0.9723, 'grad_norm': 1.1465760469436646, 'learning_rate': 9.581813647811198e-05, 'epoch': 0.44}\n",
            "{'loss': 1.0207, 'grad_norm': 0.9768258929252625, 'learning_rate': 9.431017896156074e-05, 'epoch': 0.48}\n",
            "{'loss': 0.9745, 'grad_norm': 0.8479291200637817, 'learning_rate': 9.258634670715238e-05, 'epoch': 0.52}\n",
            "{'loss': 1.0266, 'grad_norm': 0.8888425230979919, 'learning_rate': 9.065503805235138e-05, 'epoch': 0.56}\n",
            "{'loss': 0.9617, 'grad_norm': 1.0887131690979004, 'learning_rate': 8.852566213878947e-05, 'epoch': 0.6}\n",
            "{'loss': 0.9894, 'grad_norm': 0.982485294342041, 'learning_rate': 8.620859307187339e-05, 'epoch': 0.64}\n",
            "{'loss': 0.89, 'grad_norm': 1.3877309560775757, 'learning_rate': 8.371511937918616e-05, 'epoch': 0.68}\n",
            "{'loss': 0.9671, 'grad_norm': 0.8587122559547424, 'learning_rate': 8.105738901391552e-05, 'epoch': 0.72}\n",
            "{'loss': 0.8861, 'grad_norm': 1.1113277673721313, 'learning_rate': 7.82483501712469e-05, 'epoch': 0.76}\n",
            "{'loss': 0.8936, 'grad_norm': 1.3081854581832886, 'learning_rate': 7.530168820605818e-05, 'epoch': 0.8}\n",
            "{'loss': 0.9133, 'grad_norm': 1.523154377937317, 'learning_rate': 7.223175895924638e-05, 'epoch': 0.84}\n",
            "{'loss': 0.9664, 'grad_norm': 1.010873794555664, 'learning_rate': 6.905351881751372e-05, 'epoch': 0.88}\n",
            "{'loss': 0.8692, 'grad_norm': 1.0444313287734985, 'learning_rate': 6.578245184735513e-05, 'epoch': 0.92}\n",
            "{'loss': 0.8878, 'grad_norm': 1.315944790840149, 'learning_rate': 6.243449435824276e-05, 'epoch': 0.96}\n",
            "{'loss': 0.9545, 'grad_norm': 1.0025134086608887, 'learning_rate': 5.902595726252801e-05, 'epoch': 1.0}\n",
            "{'loss': 0.6845, 'grad_norm': 0.9698198437690735, 'learning_rate': 5.557344661031627e-05, 'epoch': 1.04}\n",
            "{'loss': 0.6894, 'grad_norm': 1.3703148365020752, 'learning_rate': 5.209378268645998e-05, 'epoch': 1.08}\n",
            "{'loss': 0.6696, 'grad_norm': 1.1981604099273682, 'learning_rate': 4.860391806382157e-05, 'epoch': 1.12}\n",
            "{'loss': 0.6483, 'grad_norm': 1.0546528100967407, 'learning_rate': 4.512085501204253e-05, 'epoch': 1.16}\n",
            "{'loss': 0.626, 'grad_norm': 1.7704732418060303, 'learning_rate': 4.166156266419489e-05, 'epoch': 1.2}\n",
            "{'loss': 0.681, 'grad_norm': 1.416287899017334, 'learning_rate': 3.82428943448705e-05, 'epoch': 1.24}\n",
            "{'loss': 0.6357, 'grad_norm': 3.2775824069976807, 'learning_rate': 3.488150546247778e-05, 'epoch': 1.28}\n",
            "{'loss': 0.6918, 'grad_norm': 1.5982788801193237, 'learning_rate': 3.1593772365766105e-05, 'epoch': 1.32}\n",
            "{'loss': 0.691, 'grad_norm': 1.5840332508087158, 'learning_rate': 2.8395712559900877e-05, 'epoch': 1.36}\n",
            "{'loss': 0.6582, 'grad_norm': 1.4348281621932983, 'learning_rate': 2.5302906670788462e-05, 'epoch': 1.4}\n",
            "{'loss': 0.6026, 'grad_norm': 1.269349217414856, 'learning_rate': 2.23304225378328e-05, 'epoch': 1.44}\n",
            "{'loss': 0.5874, 'grad_norm': 1.4654854536056519, 'learning_rate': 1.9492741804936622e-05, 'epoch': 1.48}\n",
            "{'loss': 0.5951, 'grad_norm': 1.2884260416030884, 'learning_rate': 1.680368936738792e-05, 'epoch': 1.52}\n",
            "{'loss': 0.6006, 'grad_norm': 1.4218820333480835, 'learning_rate': 1.4276366018359844e-05, 'epoch': 1.56}\n",
            "{'loss': 0.6123, 'grad_norm': 1.499424695968628, 'learning_rate': 1.1923084623163172e-05, 'epoch': 1.6}\n",
            "{'loss': 0.6432, 'grad_norm': 2.2602925300598145, 'learning_rate': 9.755310132204298e-06, 'epoch': 1.64}\n",
            "{'loss': 0.6577, 'grad_norm': 1.5971689224243164, 'learning_rate': 7.783603724899257e-06, 'epoch': 1.68}\n",
            "{'loss': 0.6404, 'grad_norm': 1.3056150674819946, 'learning_rate': 6.017571356669183e-06, 'epoch': 1.72}\n",
            "{'loss': 0.6529, 'grad_norm': 1.4062824249267578, 'learning_rate': 4.465816959691149e-06, 'epoch': 1.76}\n",
            "{'loss': 0.5838, 'grad_norm': 1.3504034280776978, 'learning_rate': 3.1359005254054273e-06, 'epoch': 1.8}\n",
            "{'loss': 0.6421, 'grad_norm': 1.4872568845748901, 'learning_rate': 2.0343012729971243e-06, 'epoch': 1.84}\n",
            "{'loss': 0.6679, 'grad_norm': 1.3558540344238281, 'learning_rate': 1.166386083291604e-06, 'epoch': 1.88}\n",
            "{'loss': 0.5775, 'grad_norm': 1.8757174015045166, 'learning_rate': 5.363833518505834e-07, 'epoch': 1.92}\n",
            "{'loss': 0.6372, 'grad_norm': 1.5712559223175049, 'learning_rate': 1.4736238865398765e-07, 'epoch': 1.96}\n",
            "{'loss': 0.604, 'grad_norm': 1.4710136651992798, 'learning_rate': 1.2184647302626583e-09, 'epoch': 2.0}\n",
            "100%|███████████████████████████████████████| 500/500 [3:18:34<00:00, 24.24s/it][INFO|trainer.py:4309] 2025-11-28 18:48:17,455 >> Saving model checkpoint to /kaggle/working/models/checkpoint-500\n",
            "[INFO|configuration_utils.py:765] 2025-11-28 18:48:17,790 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-11-28 18:48:17,792 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 70,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2421] 2025-11-28 18:48:18,578 >> chat template saved in /kaggle/working/models/checkpoint-500/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2590] 2025-11-28 18:48:18,579 >> tokenizer config file saved in /kaggle/working/models/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2599] 2025-11-28 18:48:18,579 >> Special tokens file saved in /kaggle/working/models/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2421] 2025-11-28 18:48:20,193 >> chat template saved in /kaggle/working/models/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2590] 2025-11-28 18:48:20,194 >> tokenizer config file saved in /kaggle/working/models/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2599] 2025-11-28 18:48:20,195 >> Special tokens file saved in /kaggle/working/models/special_tokens_map.json\n",
            "[INFO|trainer.py:2810] 2025-11-28 18:48:20,406 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 11919.3883, 'train_samples_per_second': 0.671, 'train_steps_per_second': 0.042, 'train_loss': 0.8756661472320557, 'epoch': 2.0}\n",
            "100%|███████████████████████████████████████| 500/500 [3:18:37<00:00, 23.84s/it]\n",
            "[INFO|trainer.py:5114] 2025-11-28 18:48:20,413 >> Waiting for the current checkpoint push to be finished, this might take a couple of minutes.\n",
            "[INFO|trainer.py:4309] 2025-11-28 18:48:45,965 >> Saving model checkpoint to /kaggle/working/models\n",
            "[INFO|configuration_utils.py:765] 2025-11-28 18:48:46,228 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-11-28 18:48:46,229 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 70,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2421] 2025-11-28 18:48:47,122 >> chat template saved in /kaggle/working/models/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2590] 2025-11-28 18:48:47,123 >> tokenizer config file saved in /kaggle/working/models/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2599] 2025-11-28 18:48:47,124 >> Special tokens file saved in /kaggle/working/models/special_tokens_map.json\n",
            "[INFO|trainer.py:4309] 2025-11-28 18:48:47,318 >> Saving model checkpoint to /kaggle/working/models\n",
            "[INFO|configuration_utils.py:765] 2025-11-28 18:48:47,538 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-11-28 18:48:47,539 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 70,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2421] 2025-11-28 18:48:48,440 >> chat template saved in /kaggle/working/models/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2590] 2025-11-28 18:48:48,441 >> tokenizer config file saved in /kaggle/working/models/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2599] 2025-11-28 18:48:48,442 >> Special tokens file saved in /kaggle/working/models/special_tokens_map.json\n",
            "Processing Files (0 / 0)      : |                  |  0.00B /  0.00B            \n",
            "New Data Upload               : |                  |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...adapter_model.safetensors:   9%|█▏            | 41.9MB /  479MB            \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  11%|█▌            | 53.4MB /  490MB,   ???B/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  18%|██▍           | 86.9MB /  490MB,  167MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  26%|███▋          |  129MB /  490MB,  189MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  35%|████▉         |  171MB /  490MB,  196MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  43%|██████        |  213MB /  490MB,  199MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  52%|███████▎      |  255MB /  490MB,  201MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  60%|████████▍     |  297MB /  490MB,  203MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  67%|█████████▍    |  330MB /  490MB,  198MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  76%|██████████▌   |  372MB /  490MB,  199MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  84%|███████████▊  |  414MB /  490MB,  200MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  91%|████████████▊ |  448MB /  490MB,  197MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      : 100%|█████████████▉|  490MB /  490MB,  198MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 3)      : 100%|██████████████|  490MB /  490MB,  182MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...adapter_model.safetensors: 100%|██████████████|  479MB /  479MB            \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 3)      : 100%|██████████████|  490MB /  490MB,  168MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "New Data Upload               : |                  |  0.00B /  0.00B,  0.00B/s  \n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \n",
            "  ...adapter_model.safetensors: 100%|██████████████|  479MB /  479MB            \n",
            "***** train metrics *****\n",
            "  epoch                    =        2.0\n",
            "  total_flos               = 47527668GF\n",
            "  train_loss               =     0.8757\n",
            "  train_runtime            = 3:18:39.38\n",
            "  train_samples_per_second =      0.671\n",
            "  train_steps_per_second   =      0.042\n",
            "Figure saved at: /kaggle/working/models/training_loss.png\n",
            "[WARNING|2025-11-28 18:48:56] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
            "[WARNING|2025-11-28 18:48:56] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:4643] 2025-11-28 18:48:56,181 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4645] 2025-11-28 18:48:56,181 >>   Num examples = 758\n",
            "[INFO|trainer.py:4648] 2025-11-28 18:48:56,182 >>   Batch size = 1\n",
            "100%|█████████████████████████████████████████| 758/758 [07:49<00:00,  1.61it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        2.0\n",
            "  eval_loss               =     0.8581\n",
            "  eval_runtime            = 0:07:50.18\n",
            "  eval_samples_per_second =      1.612\n",
            "  eval_steps_per_second   =      1.612\n",
            "[INFO|trainer.py:4309] 2025-11-28 18:56:46,364 >> Saving model checkpoint to /kaggle/working/models\n",
            "[INFO|configuration_utils.py:765] 2025-11-28 18:56:46,730 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-11-28 18:56:46,731 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 70,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2421] 2025-11-28 18:56:47,810 >> chat template saved in /kaggle/working/models/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2590] 2025-11-28 18:56:47,811 >> tokenizer config file saved in /kaggle/working/models/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2599] 2025-11-28 18:56:47,812 >> Special tokens file saved in /kaggle/working/models/special_tokens_map.json\n",
            "[INFO|modelcard.py:456] 2025-11-28 18:56:48,130 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "Processing Files (0 / 0)      : |                  |  0.00B /  0.00B            \n",
            "New Data Upload               : |                  |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...adapter_model.safetensors:   9%|█▏            | 41.9MB /  479MB            \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  11%|█▌            | 53.4MB /  490MB,   ???B/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  19%|██▋           | 95.3MB /  490MB,  210MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  28%|███▉          |  137MB /  490MB,  210MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  37%|█████         |  179MB /  490MB,  210MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  45%|██████▎       |  221MB /  490MB,  210MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  54%|███████▌      |  263MB /  490MB,  210MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  62%|████████▋     |  305MB /  490MB,  210MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  71%|█████████▉    |  347MB /  490MB,  210MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  79%|███████████   |  389MB /  490MB,  210MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  88%|████████████▎ |  431MB /  490MB,  210MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)      :  96%|█████████████▍|  473MB /  490MB,  210MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 3)      : 100%|██████████████|  490MB /  490MB,  199MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...adapter_model.safetensors: 100%|██████████████|  479MB /  479MB            \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...adapter_model.safetensors: 100%|██████████████|  479MB /  479MB            \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...adapter_model.safetensors: 100%|██████████████|  479MB /  479MB            \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 3)      : 100%|██████████████|  490MB /  490MB,  156MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "New Data Upload               : |                  |  0.00B /  0.00B,  0.00B/s  \n",
            "  .../models/training_args.bin: 100%|██████████████| 5.88kB / 5.88kB            \n",
            "  ...ing/models/tokenizer.json: 100%|██████████████| 11.4MB / 11.4MB            \n",
            "  ...adapter_model.safetensors: 100%|██████████████|  479MB /  479MB            \n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mcode-finetune-llamafactory\u001b[0m at: \u001b[34mhttps://wandb.ai/abdelalem999-org/llamafactory/runs/3rhyeotg\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251128_152941-3rhyeotg/logs\u001b[0m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!cd /kaggle/working/LLaMA-Factory/ && llamafactory-cli train /kaggle/working/LLaMA-Factory/examples/train_lora/code_finetune.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njgbjleSM9Og",
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
